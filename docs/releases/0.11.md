# Koheesio 0.11

Version 0.11 is Koheesio's 5th release since getting Open Sourced.

This version introduces function-based transformations as a simpler alternative to subclassing for simple 
transformations, while maintaining 100% backward compatibility. It also includes important bug fixes and improvements 
to the core Context module.

## Migrating from v0.10

For users currently using v0.10, no breaking changes have been introduced. All existing code continues to work 
unchanged. Some powerful new patterns have been added though - we encourage you to check out the new functionality.

### New Features

* **Function-Based Transformations** - Create transformations from functions without subclassing:
  * `Transformation.from_func()` - Create DataFrame-level transformations from functions
  * `ColumnsTransformation.from_func()` - Create column-level transformations with full ColumnConfig support
  * `@transformation`, `@column_transformation`, `@multi_column_transformation` decorators for Pythonic syntax
  * Reduces code by 80-95% for simple transformations
  * **As easy to write as raw PySpark, but with enterprise-grade benefits** (reusability, type safety, testability)
  * See the [Function-Based Transformations tutorial](../tutorials/function-based-transformations.md) for examples

### Deprecations

* **Transform Class** - The `Transform` class is now deprecated in favor of `Transformation.from_func()`:
* **ColumnsTransformationWithTarget** - The `ColumnsTransformationWithTarget` class is now deprecated in favor of
  the `ColumnsTransformation` class:

Both will be removed in v1.0. See migration examples in the tutorial


## Release 0.11.0

**v0.11.0** - *TBD*

* **Full Changelog**:
    <https://github.com/Nike-Inc/koheesio/compare/koheesio-v0.10.6...koheesio-v0.11.0>

### Features

!!! feat "feature - PR [#TBD](https://github.com/Nike-Inc/koheesio/pull/TBD), related discussion [#225](https://github.com/Nike-Inc/koheesio/discussions/225)"
    #### *Spark > Transformations*: Function-Based Transformations

    Introduced function-based transformations as a simpler alternative to subclassing for simple transformations.

    **New Methods:**

    * `Transformation.from_func(func, **kwargs)` - Create DataFrame-level transformations from functions
    * `ColumnsTransformation.from_func(func, for_each=None, **kwargs)` - Create column-level transformations (defaults to column-wise)
    * `ColumnsTransformation.from_multi_column_func(func, for_each=None, **kwargs)` - Create multi-column transformations (defaults to multi-column)

    **Benefits:**

    * **Code Reduction**: 80-95% less code for simple transformations
    * **Easier to Write**: No class definitions needed for simple cases
    * **Factory Patterns**: Create related transformations easily
    * **Full ColumnConfig Support**: Type validation and automatic column selection
    * **Backward Compatible**: All existing code continues to work

    **Before (v0.10) - Subclassing:**
    ```python
    from koheesio.spark.transformations import ColumnsTransformationWithTarget
    from koheesio.spark.utils import SparkDatatype
    from pyspark.sql import functions as f
    
    class LowerCase(ColumnsTransformationWithTarget):
        class ColumnConfig:
            run_for_all_data_type = [SparkDatatype.STRING]
            limit_data_type = [SparkDatatype.STRING]
        
        def func(self, column):
            return f.lower(column)
    
    # Usage
    output_df = LowerCase(columns=["name"]).transform(input_df)
    ```

    **After (v0.11) - Function-Based:**
    ```python
    from koheesio.spark.transformations import ColumnsTransformation
    from koheesio.spark.utils import SparkDatatype
    from pyspark.sql import functions as f
    
    LowerCase = ColumnsTransformation.from_func(
        lambda col: f.lower(col),
        run_for_all_data_type=[SparkDatatype.STRING],
        limit_data_type=[SparkDatatype.STRING]
    )
    
    # Usage (same as before)
    output_df = LowerCase(columns=["name"]).transform(input_df)
    ```

    **Factory Pattern Example:**
    ```python
    # v0.10: Would need 3 separate class definitions
    # v0.11: Create multiple transformations in 4 lines
    
    def string_transform(func):
        return ColumnsTransformation.from_func(
            func,
            run_for_all_data_type=[SparkDatatype.STRING],
            limit_data_type=[SparkDatatype.STRING]
        )
    
    LowerCase = string_transform(lambda col: f.lower(col))
    UpperCase = string_transform(lambda col: f.upper(col))
    TitleCase = string_transform(lambda col: f.initcap(col))
    ```

    **DataFrame Transformation Example:**
    ```python
    # v0.10: Would use Transform class (now deprecated)
    # v0.11: Use Transformation.from_func()
    
    def lowercase_columns(df):
        return df.select([f.col(c).alias(c.lower()) for c in df.columns])
    
    LowercaseColumns = Transformation.from_func(lowercase_columns)
    output_df = LowercaseColumns().transform(input_df)
    ```

    **Multi-Column Aggregation Example:**
    ```python
    # Sum multiple columns into one
    from pyspark.sql import Column
    
    def sum_quarters(q1: Column, q2: Column, q3: Column, q4: Column) -> Column:
        return q1 + q2 + q3 + q4
    
    # Option A: from_func() - auto-detects multi-column from type hints
    SumQuarters = ColumnsTransformation.from_func(sum_quarters)
    output_df = SumQuarters(
        columns=["sales_q1", "sales_q2", "sales_q3", "sales_q4"],
        target_column="total_sales"
    ).transform(input_df)
    # Result: Passes all 4 columns to function at once
    
    # Option B: from_multi_column_func() - makes intent explicit
    SumQuarters = ColumnsTransformation.from_multi_column_func(sum_quarters)
    # Same behavior, but method name makes it clear this is multi-column
    
    # Concatenate columns
    def concat_names(first: Column, last: Column) -> Column:
        return f.concat(first, f.lit(" "), last)
    
    ConcatNames = ColumnsTransformation.from_func(concat_names)
    output_df = ConcatNames(
        columns=["first_name", "last_name"],
        target_column="full_name"
    ).transform(input_df)
    ```

    **Paired Parameters Example (Different Values Per Column):**
    ```python
    # Apply different tax rates to different columns
    def add_tax(amount: Column, rate: float = 0.08) -> Column:
        return amount * (1 + rate)
    
    AddTax = ColumnsTransformation.from_func(add_tax)
    
    # Single rate for all columns
    output_df = AddTax(columns=["price", "cost"], rate=0.08).transform(input_df)
    
    # Different rate per column (paired parameters)
    output_df = AddTax(
        columns=["food_price", "non_food_price"],
        rate=[0.08, 0.13]  # Must match column count (strict=True)
    ).transform(input_df)
    # Result: food_price gets 8% tax, non_food_price gets 13% tax
    
    # Multiple paired parameters
    def scale_and_offset(col: Column, scale: float, offset: float) -> Column:
        return col * scale + offset
    
    ScaleOffset = ColumnsTransformation.from_func(scale_and_offset)
    ScaleOffset(
        columns=["temp_c", "pressure_kpa"],
        scale=[1.8, 0.145],      # Celsius to Fahrenheit, kPa to PSI
        offset=[32.0, 0.0]
    )
    ```

    **Important:** List parameters must exactly match the number of columns (like Python's `zip(..., strict=True)`). If you need the same value for all columns, pass a single value instead of a list.

    **Variadic Support (Any Number of Columns):**
    ```python
    from functools import reduce
    
    # Variadic *args - any number of columns
    def sum_columns(*cols: Column) -> Column:
        return reduce(lambda a, b: a + b, cols)
    
    SumColumns = ColumnsTransformation.from_func(sum_columns)
    
    # Works with any number of columns
    SumColumns(columns=["jan", "feb", "mar"], target_column="q1_total")
    SumColumns(columns=["q1", "q2", "q3", "q4"], target_column="yearly_total")
    ```

    **Two Methods for Different Defaults:**
    
    - `from_func()` - Defaults to column-wise (for_each=True) when ambiguous
    - `from_multi_column_func()` - Defaults to multi-column (for_each=False) when ambiguous
    - Both methods auto-detect from type hints when available
    - Use `for_each` parameter to explicitly override: `from_func(func, for_each=False)`
    
    **Pattern Detection:**
    
    The implementation automatically detects the pattern based on function signature:
    
    | Function Signature | Detected Pattern | from_func() | from_multi_column_func() |
    |-------------------|------------------|-------------|--------------------------|
    | `func(col: Column)` | Column-wise (auto) | ✅ Column-wise | ✅ Column-wise (overrides default) |
    | `func(c1: Column, c2: Column)` | Multi-column (auto) | ✅ Multi-column (overrides default) | ✅ Multi-column |
    | `func(*cols: Column)` | Multi-column (auto) | ✅ Multi-column | ✅ Multi-column |
    | `func(col: Column, rate: float)` | Column-wise (auto) | ✅ Column-wise | ✅ Column-wise (overrides default) |
    | `lambda col: ...` | Ambiguous | ⚠️ Column-wise (default) | ⚠️ Multi-column (default) |
    | `lambda a, b: ...` | Ambiguous | ⚠️ Column-wise (default) | ⚠️ Multi-column (default) |
    
    **Recommendation:** Use type hints for auto-detection, or choose the method that matches your intent for lambdas.

    **When to Use Function-Based:**

    * Simple column operations (0-2 parameters, pure functions)
    * DataFrame transformations without complex validation
    * Factory patterns for related transformations

    **When to Keep Subclassing:**

    * Complex validation logic
    * Computed properties or multiple methods
    * Orchestration of other transformations

    **Implementation Note:**

    ColumnConfig parameters (`run_for_all_data_type`, `limit_data_type`, `data_type_strict_mode`) are now also available as Pydantic fields on `ColumnsTransformation`, allowing function-based transformations to leverage them naturally while maintaining 100% backward compatibility with existing subclasses that use the nested `ColumnConfig` class.

    **Looking Ahead to v1.0:**

    This release lays the groundwork for v1.0, where function-based transformations will become the primary pattern for simple transformations. In v1.0, we plan to:
    
    * Remove the deprecated `Transform` and `ColumnsTransformationWithTarget` classes (functionality is now available via `.from_func()` methods)
    * Migrate Koheesio's built-in simple transformations to use `.from_func()` (or decorator patterns) where appropriate 
    * Remove the nested `ColumnConfig` class in favor of the field-based approach

    For more details on the design and rationale, see [RFC #225](https://github.com/Nike-Inc/koheesio/discussions/225).

    See the [Function-Based Transformations tutorial](../tutorials/function-based-transformations.md) for comprehensive examples and migration guidance.

    <small> by @dannymeijer </small>

!!! feat "feature - PR [#TBD](https://github.com/Nike-Inc/koheesio/pull/TBD), related discussion [#225](https://github.com/Nike-Inc/koheesio/discussions/225)"
    #### *Spark > Transformations*: Decorator-Based Transformations with Type Safety

    Added decorator-based syntax for defining Spark transformations, built on top of the existing `.from_func()` APIs
    with Pydantic-powered runtime validation.

    **New decorators:**

    - `@transformation` – DataFrame-level transformations with parameter validation.
    - `@column_transformation` – Column-level transformations with full ColumnConfig support.
    - `@multi_column_transformation` – Multi-column aggregations (N→1) with type validation.

    **Why it matters:**

    - Clear, declarative intent (e.g. `@column_transformation` signals column-level ops).
    - Reusable, testable transformation classes instead of ad‑hoc helper functions.
    - Shared patterns and validation across teams.

    For examples and advanced usage (paired parameters, variadic functions, factory patterns), see the
    [Decorator-Based Transformations reference](../reference/spark/transformations.md#decorator-based-transformations).

    <small> by @dannymeijer </small>

!!! info "Why Choose Koheesio Over Raw PySpark?"
    #### The Value Proposition

    With v0.11's decorator-based transformations, **Koheesio is now as easy to write as raw PySpark**, but provides 
    enterprise-grade benefits that become critical in team environments and production pipelines:

    **1. Reusability & Team Libraries**
    ```python
    # Raw PySpark: Copy-paste everywhere
    df = df.withColumn("name", f.upper(f.col("name")))
    df = df.withColumn("city", f.upper(f.col("city")))
    
    # Koheesio: Define once, reuse everywhere
    @column_transformation(run_for_all_data_type=[SparkDatatype.STRING])
    def to_upper(col: Column) -> Column:
        return f.upper(col)
    
    to_upper().transform(df)  # Applies to ALL string columns
    ```

    **2. Fail-Fast Error Detection**
    ```python
    # Raw PySpark: Silent data corruption
    for col, rate in zip(columns, rates):  # Silently truncates!
        df = df.withColumn(col, f.col(col) * (1 + rate))
    
    # Koheesio: Clear errors (strict zip semantics)
    AddTax(columns=["a", "b", "c"], rate=[0.08, 0.13])
    # ValueError: Parameter 'rate' has 2 values but 3 columns provided.
    ```

    **3. Easy Unit Testing**
    ```python
    # Koheesio transformations are independently testable
    @column_transformation
    def add_tax(amount: Column, rate: float) -> Column:
        return amount * (1 + rate)
    
    def test_add_tax(spark):
        df = spark.createDataFrame([(100.0,)], ["price"])
        result = add_tax(columns=["price"], rate=0.08).transform(df)
        assert result.select("price").first()[0] == 108.0
    ```

    **4. Self-Documenting Code**
    ```python
    # Intent is crystal clear
    @multi_column_transformation
    def sum_quarters(q1: Column, q2: Column, q3: Column, q4: Column) -> Column:
        """Sum quarterly sales into annual total."""
        return q1 + q2 + q3 + q4
    
    SumQuarters(
        columns=["sales_q1", "sales_q2", "sales_q3", "sales_q4"],
        target_column="annual_sales"
    )
    ```

    **5. Factory Patterns (DRY)**
    ```python
    # Create 3 transformations in 6 lines
    def string_transform(func):
        return ColumnsTransformation.from_func(
            func, run_for_all_data_type=[SparkDatatype.STRING]
        )
    
    LowerCase = string_transform(lambda col: f.lower(col))
    UpperCase = string_transform(lambda col: f.upper(col))
    TitleCase = string_transform(lambda col: f.initcap(col))
    ```

    **6. Team Consistency**
    
    Everyone uses the same standardized patterns, making code reviews easier and onboarding faster.

    **The Key Insight:** Decorators lower the barrier to entry to raw PySpark levels, while providing reusability, type safety, ColumnConfig automation, testability, and team consistency for free.

    **When to Use Raw PySpark:** One-off scripts, extremely simple operations, performance-critical hot paths, or solo development where team consistency isn't needed.

### Deprecations

!!! warning "deprecation - PR [#TBD](https://github.com/Nike-Inc/koheesio/pull/TBD), related discussion [#225](https://github.com/Nike-Inc/koheesio/discussions/225)"
    #### *Spark > Transformations*: Transform Class Deprecated

    The `Transform` class is now deprecated in favor of `Transformation.from_func()`.

    **Migration:**
    ```python
    # Old (v0.10) - using Transform
    from koheesio.spark.transformations.transform import Transform
    
    transform = Transform(func=some_func, a="foo", b="bar")
    
    # New (v0.11+) - using Transformation.from_func()
    from koheesio.spark.transformations import Transformation
    
    SomeFunc = Transformation.from_func(some_func)
    transform = SomeFunc(a="foo", b="bar")
    ```

    **Timeline:**

    * v0.11: Deprecation warning shown when using `Transform`
    * v1.0: `Transform` class will be removed

    <small> by @dannymeijer </small>

!!! warning "deprecation - PR [#TBD](https://github.com/Nike-Inc/koheesio/pull/TBD), related discussion [#225](https://github.com/Nike-Inc/koheesio/discussions/225)"
    #### *Spark > Transformations*: ColumnsTransformationWithTarget Deprecated

    The `ColumnsTransformationWithTarget` class is now deprecated in favor of `ColumnsTransformation.from_func()`.

    **Migration:**
    ```python
    # Old (v0.10) - using ColumnsTransformationWithTarget
    from koheesio.spark.transformations import ColumnsTransformationWithTarget
    from pyspark.sql import functions as f
    
    class MyTransform(ColumnsTransformationWithTarget):
        def func(self, col):
            return f.lower(col)
    
    # New (v0.11+) - using ColumnsTransformation.from_func()
    from koheesio.spark.transformations import ColumnsTransformation
    from pyspark.sql import functions as f
    
    MyTransform = ColumnsTransformation.from_func(lambda col: f.lower(col))
    ```

    **Timeline:**

    * v0.11: Deprecation warning shown when subclassing `ColumnsTransformationWithTarget`
    * v1.0: `ColumnsTransformationWithTarget` class will be removed

    <small> by @dannymeijer </small>

!!! warning "deprecation - PR [#TBD](https://github.com/Nike-Inc/koheesio/pull/TBD), related discussion [#225](https://github.com/Nike-Inc/koheesio/discussions/225)"
    #### *Spark > Transformations*: Nested ColumnConfig Deprecated

    Using nested `ColumnConfig` classes is now deprecated in favor of using the field-based approach.

    **Migration:**
    ```python
    # Old (v0.10) - using nested ColumnConfig
    from koheesio.spark.transformations import ColumnsTransformation
    from koheesio.spark.utils import SparkDatatype
    
    class MyTransform(ColumnsTransformation):
        class ColumnConfig:
            run_for_all_data_type = [SparkDatatype.STRING]
            limit_data_type = [SparkDatatype.STRING]
        
        def execute(self):
            # transformation logic
            pass
    
    # New (v0.11+) - using field-based approach
    from koheesio.spark.transformations import ColumnsTransformation
    from koheesio.spark.utils import SparkDatatype
    
    MyTransform = ColumnsTransformation.from_func(
        lambda col: f.lower(col),
        run_for_all_data_type=[SparkDatatype.STRING],
        limit_data_type=[SparkDatatype.STRING]
    )
    ```

    **Timeline:**

    * v0.11: Deprecation warning shown when nested `ColumnConfig` is accessed
    * v1.0: Nested `ColumnConfig` class will be removed

    <small> by @dannymeijer </small>

### Bugfixes

!!! bug "bugfix - PR [#TBD](https://github.com/Nike-Inc/koheesio/pull/TBD), related issue [#226](https://github.com/Nike-Inc/koheesio/issues/226)"
    #### *Core > Context*: Handle Path objects gracefully in `Context.from_yaml()`

    Fixed a bug where `Context.from_yaml()` would raise a confusing `AttributeError: 'PosixPath' object has no attribute 'read'` when passed a `pathlib.Path` object pointing to a non-existent file.

    **Root Cause:**  
    When a Path object for a non-existent file was passed to `from_yaml()`, the method would skip reading the file (since it doesn't exist), but would then pass the Path object directly to `yaml.load()`. The YAML parser tried to call `.read()` on the Path object, causing an AttributeError.

    **Fix:**  
    Added an `else` clause to convert Path objects to strings when the file doesn't exist, matching the existing pattern used in `Context.from_toml()`. This prevents the AttributeError and allows the YAML parser to handle the stringified path appropriately.

    **Impact:**  
    This bug was commonly encountered where configuration files might be missing due to environment misconfiguration. The error message was confusing and didn't clearly indicate that a file was missing. Now, the error handling is more graceful and consistent with the `from_toml()` method.

    **Test Coverage:**  
    Added comprehensive test (`test_from_yaml_path_object_nonexistent_file`) to verify the fix and prevent regression.

    <small> by @dannymeijer </small>

!!! bug "bugfix - PR [#TBD](https://github.com/Nike-Inc/koheesio/pull/TBD), related issue [#222](https://github.com/Nike-Inc/koheesio/issues/222)"
    #### *Core > Models*: Fix `ListOfColumns` validation with PySpark Column objects

    Fixed a critical bug where `ListOfColumns` validation would fail when PySpark `Column` objects were passed in a list, raising `CANNOT_CONVERT_COLUMN_INTO_BOOL` error.

    **Root Cause:**  
    The `_list_of_columns_validation` function used `if col` to filter out empty values, which attempted to convert PySpark Column objects to boolean. PySpark explicitly prohibits this conversion to prevent ambiguous boolean operations in DataFrame expressions.

    **Fix:**  
    Replaced the implicit boolean check with explicit type checking:
    ```python
    # Before (broken):
    columns = [col for col in columns if col]  # ❌ Triggers CANNOT_CONVERT_COLUMN_INTO_BOOL
    
    # After (fixed):
    for col in columns:
        if isinstance(col, Column):  # ✅ Check type first
            filtered_columns.append(col)
        elif col is not None and col != "":  # ✅ Explicit checks for strings
            filtered_columns.append(col)
    ```

    **Impact:**  
    This bug affected real-world usage in join operations and transformations where Column expressions with aliases are commonly used. The `ListOfColumns` type now properly supports both string column names and PySpark Column objects as originally intended.

    **Note:**  
    The `_list_of_columns_validation` helper has been moved out of the `koheesio.models` internals into the shared Spark
    utilities module. It now lives in `koheesio.spark.utils.common` and is wired into the `ListOfColumns` annotated
    type, which is re-exported via `koheesio.spark.utils`. Code that previously imported `ListOfColumns` or
    `_list_of_columns_validation` from other modules (e.g. `koheesio.models`) should update imports to use
    `koheesio.spark.utils.ListOfColumns` or the new helper location as appropriate.

    **Test Coverage:**  
    Verified through existing transformation tests that use Column objects in various scenarios.

    <small> by @dannymeijer </small>
